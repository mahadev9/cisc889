{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Code: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import glob\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['race_id', 'circuit_id', 'year', 'round', 'race_length', 'driver_id', 'lap', 'position', 'milliseconds', 'pit_stop_count', 'pit_stop_milliseconds', 'pit_stop', 'rating', 'fcy', 'battle', 'next_lap_time', 'fitted_tire', 'tire_age', 'starting_position', 'gap_from_the_car_in_front', 'gap_from_the_following_car', 'lap_time_of_the_car_in_front', 'lap_time_of_the_following_car', 'drs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYRE_MAP = {'HYPERSOFT': 1, 'ULTRASOFT': 1, 'SUPERSOFT': 1, 'SOFT': 1, 'MEDIUM': 2, 'HARD': 3, 'INTERMEDIATE': 4, 'WET': 5, 'nan': 0}\n",
    "NUM_TYRES = 5\n",
    "ROUND_MAP = {'british': 1, 'austrian': 2, 'brazilian': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3687, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = 'british'\n",
    "files = glob.glob(f'data/new/*_{gp}_data.csv')\n",
    "files.sort()\n",
    "\n",
    "# Load data\n",
    "dataFrame = pd.concat([pd.read_csv(f, names=headers) for f in files], ignore_index=True)\n",
    "dataFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>race_length</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>lap</th>\n",
       "      <th>position</th>\n",
       "      <th>milliseconds</th>\n",
       "      <th>pit_stop_count</th>\n",
       "      <th>pit_stop_milliseconds</th>\n",
       "      <th>pit_stop</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>battle</th>\n",
       "      <th>next_lap_time</th>\n",
       "      <th>fitted_tire</th>\n",
       "      <th>tire_age</th>\n",
       "      <th>starting_position</th>\n",
       "      <th>gap_from_the_car_in_front</th>\n",
       "      <th>gap_from_the_following_car</th>\n",
       "      <th>lap_time_of_the_car_in_front</th>\n",
       "      <th>lap_time_of_the_following_car</th>\n",
       "      <th>drs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92918.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>92723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93267.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92723.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93616.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93108.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>92991.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93653.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92991.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93359.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5312.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93494.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93359.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93315.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93703.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   round race_length driver_id  lap position milliseconds pit_stop_count  \\\n",
       "3    1.0          52         5  3.0      1.0      92918.0              2   \n",
       "4    1.0          52         5  4.0      1.0      92723.0              2   \n",
       "5    1.0          52         5  5.0      1.0      93108.0              2   \n",
       "6    1.0          52         5  6.0      1.0      92991.0              2   \n",
       "7    1.0          52         5  7.0      1.0      93359.0              2   \n",
       "\n",
       "  pit_stop_milliseconds pit_stop rating  ... battle next_lap_time fitted_tire  \\\n",
       "3                   0.0        0   25.0  ...      0       92723.0         1.0   \n",
       "4                   0.0        0   25.0  ...      0       93108.0         1.0   \n",
       "5                   0.0        0   25.0  ...      0       92991.0         1.0   \n",
       "6                   0.0        0   25.0  ...      0       93359.0         1.0   \n",
       "7                   0.0        0   25.0  ...      0       93315.0         1.0   \n",
       "\n",
       "   tire_age starting_position gap_from_the_car_in_front  \\\n",
       "3       4.0               1.0                       0.0   \n",
       "4       5.0               1.0                       0.0   \n",
       "5       6.0               1.0                       0.0   \n",
       "6       7.0               1.0                       0.0   \n",
       "7       8.0               1.0                       0.0   \n",
       "\n",
       "  gap_from_the_following_car lap_time_of_the_car_in_front  \\\n",
       "3                     4007.0                          0.0   \n",
       "4                     4515.0                          0.0   \n",
       "5                     5177.0                          0.0   \n",
       "6                     5312.0                          0.0   \n",
       "7                     5700.0                          0.0   \n",
       "\n",
       "  lap_time_of_the_following_car drs  \n",
       "3                       93267.0   0  \n",
       "4                       93616.0   0  \n",
       "5                       93653.0   0  \n",
       "6                       93494.0   0  \n",
       "7                       93703.0   0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame['fitted_tire'] = dataFrame['fitted_tire'].map(TYRE_MAP)\n",
    "dataFrame['round'] = dataFrame['round'].map(ROUND_MAP)\n",
    "dataFrame = dataFrame.drop(columns=['race_id', 'circuit_id', 'year'])\n",
    "dataFrame = dataFrame.dropna()\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['milliseconds'] = dataFrame['milliseconds'].astype(float)\n",
    "dataFrame['next_lap_time'] = dataFrame['next_lap_time'].astype(float)\n",
    "dataFrame['lap_time_of_the_car_in_front'] = dataFrame['lap_time_of_the_car_in_front'].astype(float)\n",
    "dataFrame['lap_time_of_the_following_car'] = dataFrame['lap_time_of_the_following_car'].astype(float)\n",
    "dataFrame['gap_from_the_car_in_front'] = dataFrame['gap_from_the_car_in_front'].astype(float)\n",
    "dataFrame['gap_from_the_following_car'] = dataFrame['gap_from_the_following_car'].astype(float)\n",
    "dataFrame['lap'] = dataFrame['lap'].astype(float).astype(int)\n",
    "dataFrame['position'] = dataFrame['position'].astype(float).astype(int)\n",
    "dataFrame['pit_stop_count'] = dataFrame['pit_stop_count'].astype(int)\n",
    "dataFrame['pit_stop_milliseconds'] = dataFrame['pit_stop_milliseconds'].astype(float)\n",
    "dataFrame['pit_stop'] = dataFrame['pit_stop'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['milliseconds'] = dataFrame['milliseconds'] / 1000\n",
    "dataFrame['next_lap_time'] = dataFrame['next_lap_time'] / 1000\n",
    "dataFrame['pit_stop_milliseconds'] = dataFrame['pit_stop_milliseconds'] / 1000\n",
    "dataFrame['gap_from_the_car_in_front'] = dataFrame['gap_from_the_car_in_front'] / 1000\n",
    "dataFrame['gap_from_the_following_car'] = dataFrame['gap_from_the_following_car'] / 1000\n",
    "dataFrame['lap_time_of_the_car_in_front'] = dataFrame['lap_time_of_the_car_in_front'] / 1000\n",
    "dataFrame['lap_time_of_the_following_car'] = dataFrame['lap_time_of_the_following_car'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaOneRacingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Markov Decision Process model for Formula 1 race strategy, incorporating detailed race data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(FormulaOneRacingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.current_index = 1\n",
    "\n",
    "        self.lap_count = int(self.df['race_length'].max())  # Total laps\n",
    "\n",
    "        # Define the action space\n",
    "        # Action 0: No pit stop\n",
    "        # Action 1-5: Pit stop and change to soft, medium, hard, intermediates, wets\n",
    "        self.num_tire_types = NUM_TYRES\n",
    "        self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(self.lap_count),  # Lap number to take action\n",
    "                                          gym.spaces.Discrete(self.num_tire_types + 1)))  # Tire type including no action\n",
    "        \n",
    "        self.optimal_lap_time = self.df['milliseconds'].mean()  # Assume the fastest lap time is the optimal lap time\n",
    "        self.optimal_pit_stop_time = self.df['pit_stop_milliseconds'].mean()  # Assume the average pit stop time is the optimal pit stop time\n",
    "\n",
    "        # Define the state space\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'driver_id': gym.spaces.Discrete(len(self.df['driver_id'].unique())),\n",
    "            'lap': gym.spaces.Discrete(self.lap_count),\n",
    "            'position': gym.spaces.Discrete(20),\n",
    "            'milliseconds': gym.spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'pit_stop_count': gym.spaces.Discrete(10),\n",
    "            'pit_stop_milliseconds': gym.spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'pit_stop': gym.spaces.Discrete(2),\n",
    "            'fcy': gym.spaces.Discrete(2),\n",
    "            'battle': gym.spaces.Discrete(2),\n",
    "            'next_lap_time': gym.spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'fitted_tire': gym.spaces.Discrete(self.num_tire_types),\n",
    "            'tire_age': gym.spaces.Discrete(100),\n",
    "            'starting_position': gym.spaces.Discrete(20),\n",
    "            'gap_from_the_car_in_front': gym.spaces.Box(low=-float('inf'), high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'gap_from_the_following_car': gym.spaces.Box(low=-float('inf'), high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'lap_time_of_the_car_in_front': gym.spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'lap_time_of_the_following_car': gym.spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            'drs': gym.spaces.Discrete(2)\n",
    "        })\n",
    "\n",
    "    def set_state_from_data(self):\n",
    "        # Set the environment's state based on the current index of the dataframe\n",
    "        row = self.df.iloc[self.current_index]\n",
    "        self.state = {\n",
    "            'lap': row['lap'],\n",
    "            'position': row['position'],\n",
    "            'milliseconds': float(row['milliseconds']),\n",
    "            'pit_stop_count': int(row['pit_stop_count']),\n",
    "            'pit_stop_milliseconds': row['pit_stop_milliseconds'],\n",
    "            'pit_stop': row['pit_stop'],\n",
    "            'fcy': row['fcy'],\n",
    "            'battle': row['battle'],\n",
    "            'next_lap_time': row['next_lap_time'],\n",
    "            'fitted_tire': row['fitted_tire'],\n",
    "            'tire_age': int(float(row['tire_age'])),\n",
    "            'starting_position': row['starting_position'],\n",
    "            'gap_from_the_car_in_front': row['gap_from_the_car_in_front'],\n",
    "            'gap_from_the_following_car': row['gap_from_the_following_car'],\n",
    "            'lap_time_of_the_car_in_front': row['lap_time_of_the_car_in_front'],\n",
    "            'lap_time_of_the_following_car': row['lap_time_of_the_following_car'],\n",
    "            'drs': row['drs']\n",
    "        }\n",
    "        self.current_index += 1  # Move to the next row for the next step\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.reward_system(action)\n",
    "\n",
    "        # Transition to the next state\n",
    "        self.current_index += 1\n",
    "        if self.current_index >= len(self.df):\n",
    "            done = True\n",
    "        else:\n",
    "            self.set_state_from_data()\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 1\n",
    "        self.set_state_from_data()\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def reward_system(self, action):\n",
    "        lap_decision, tire_choice = action\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the action corresponds to making a pit stop on the current lap\n",
    "        if self.state['lap'] == lap_decision and tire_choice != 0:\n",
    "            # Update state to reflect the pit stop\n",
    "            self.state['pit_stop_count'] += 1\n",
    "            self.state['tire_age'] = 0  # Reset tire age\n",
    "\n",
    "            # Calculate penalty based on the tire choice and other pit stop dynamics\n",
    "            reward -= self.calculate_pit_stop_time_penalty(tire_choice)\n",
    "        else:\n",
    "            # Calculate the reward based on lap time if no pit stop is made\n",
    "            reward += self.calculate_lap_time_reward()\n",
    "\n",
    "        # Optionally, apply additional penalties for each pit stop\n",
    "        reward -= self.state['pit_stop_count']  # Example penalty for each pit stop made\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def calculate_pit_stop_time_penalty(self, tire_choice):\n",
    "        if tire_choice == self.state['fitted_tire'] and not self.state['pit_stop']:\n",
    "            return 0\n",
    "        tire_penalty = {\n",
    "            1: 10,  # Soft tires\n",
    "            2: 20,  # Medium tires\n",
    "            3: 30,  # Hard tires\n",
    "            4: 50,  # Intermediates\n",
    "            5: 80   # Wets\n",
    "        }\n",
    "        if self.optimal_pit_stop_time > self.state['pit_stop_milliseconds']:\n",
    "            time_penalty = self.optimal_pit_stop_time + self.state['pit_stop_milliseconds'] * 0.6\n",
    "        else:\n",
    "            time_penalty = self.optimal_pit_stop_time + tire_penalty.get(tire_choice, 0)\n",
    "        return time_penalty / 1000  # Scale the penalty\n",
    "    \n",
    "    def calculate_lap_time_reward(self):\n",
    "        current_lap_time = self.state['milliseconds']\n",
    "\n",
    "        # Reward is inversely proportional to the difference from optimal lap time\n",
    "        time_difference = self.optimal_lap_time - current_lap_time\n",
    "        reward = time_difference / 1000  # Scale the reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_laps, num_tire_choices, hidden_dim=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, num_laps * num_tire_choices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state_dict):\n",
    "    # Flatten the dictionary to a single list or array\n",
    "    flat_state = np.array(list(state_dict.values()), dtype=np.float32)\n",
    "    return flat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=5e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01, buffer_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.num_tire_choices = action_dim[1]\n",
    "\n",
    "        self.q_network = QNetwork(state_dim, self.action_dim[0], self.action_dim[1])\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Ensure all elements are numeric or properly converted\n",
    "        state = [float(x) for x in state.values()]  # Assuming state is a dict of numeric-ready values\n",
    "        next_state = [float(x) for x in next_state.values()]\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        state = torch.FloatTensor(process_state(state)).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.q_network(state)\n",
    "        \n",
    "        # Reshape the output to be a 2D matrix of shape (num_laps, num_tire_choices)\n",
    "        action_values = action_values.view(-1, self.action_dim[1])  # self.action_dim[1] is num_tire_choices\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            # Use argmax to find the index of the maximum Q-value\n",
    "            flat_index = action_values.argmax()  # Get the flat index of the max Q-value\n",
    "            lap_decision = flat_index // self.action_dim[1]  # Convert flat index to 2D index (lap)\n",
    "            tire_choice = flat_index % self.action_dim[1]  # Convert flat index to 2D index (tire)\n",
    "        else:\n",
    "            # Random selection for exploration\n",
    "            lap_decision = random.randint(0, self.action_dim[0] - 1)\n",
    "            tire_choice = random.randint(0, self.action_dim[1] - 1)\n",
    "\n",
    "        return lap_decision, tire_choice\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        actions = torch.LongTensor([action[0] * self.num_tire_choices + action[1] for action in actions])\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Ensure actions are unsqueezed to use as indices\n",
    "        actions = actions.unsqueeze(1)\n",
    "\n",
    "        # Gather Q-values for the chosen actions\n",
    "        current_q_values = self.q_network(states).gather(1, actions).squeeze()\n",
    "        next_q_values = self.q_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Total Reward = -2946.522775933589, Epsilon = 0.9511101304657719\n",
      "Episode 20: Total Reward = -2944.5565578071405, Epsilon = 0.9046104802746175\n",
      "Episode 30: Total Reward = -2946.6935809536017, Epsilon = 0.8603841919146962\n",
      "Episode 40: Total Reward = -2953.4595640559796, Epsilon = 0.8183201210226743\n",
      "Episode 50: Total Reward = -2954.6001880391586, Epsilon = 0.778312557068642\n",
      "Episode 60: Total Reward = -2935.630689758416, Epsilon = 0.7402609576967045\n",
      "Episode 70: Total Reward = -2940.5970497543553, Epsilon = 0.7040696960536299\n",
      "Episode 80: Total Reward = -2942.57291780076, Epsilon = 0.6696478204705644\n",
      "Episode 90: Total Reward = -2946.4860529736125, Epsilon = 0.6369088258938781\n",
      "Episode 100: Total Reward = -2947.5376168767452, Epsilon = 0.6057704364907278\n",
      "Episode 110: Total Reward = -2954.3869081592284, Epsilon = 0.5761543988830038\n",
      "Episode 120: Total Reward = -2954.4944750991926, Epsilon = 0.547986285490042\n",
      "Episode 130: Total Reward = -2950.519243826282, Epsilon = 0.5211953074858876\n",
      "Episode 140: Total Reward = -2951.60131806961, Epsilon = 0.49571413690105054\n",
      "Episode 150: Total Reward = -2941.5550208576046, Epsilon = 0.47147873742168567\n",
      "Episode 160: Total Reward = -2937.5621617647967, Epsilon = 0.4484282034609769\n",
      "Episode 170: Total Reward = -2952.552926952732, Epsilon = 0.42650460709830135\n",
      "Episode 180: Total Reward = -2937.5457407848085, Epsilon = 0.40565285250151817\n",
      "Episode 190: Total Reward = -2932.556272668799, Epsilon = 0.3858205374665315\n",
      "Episode 200: Total Reward = -2934.5744037352138, Epsilon = 0.3669578217261671\n",
      "Episode 210: Total Reward = -2955.5706370423486, Epsilon = 0.34901730169741024\n",
      "Episode 220: Total Reward = -2956.4501250455382, Epsilon = 0.33195389135223546\n",
      "Episode 230: Total Reward = -2946.5590578135207, Epsilon = 0.3157247089126454\n",
      "Episode 240: Total Reward = -2953.575965136026, Epsilon = 0.30028896908517405\n",
      "Episode 250: Total Reward = -2958.606550292058, Epsilon = 0.285607880564032\n",
      "Episode 260: Total Reward = -2954.4023120991933, Epsilon = 0.27164454854530906\n",
      "Episode 270: Total Reward = -2959.603106175179, Epsilon = 0.2583638820072446\n",
      "Episode 280: Total Reward = -2951.590134069611, Epsilon = 0.2457325055235537\n",
      "Episode 290: Total Reward = -2957.6382960487285, Epsilon = 0.23371867538818816\n",
      "Episode 300: Total Reward = -2956.5708350455398, Epsilon = 0.22229219984074702\n",
      "Episode 310: Total Reward = -2948.6020149799942, Epsilon = 0.21142436319205632\n",
      "Episode 320: Total Reward = -2949.350346083242, Epsilon = 0.2010878536592394\n",
      "Episode 330: Total Reward = -2956.621767145596, Epsilon = 0.1912566947289212\n",
      "Episode 340: Total Reward = -2933.5461966719895, Epsilon = 0.18190617987607657\n",
      "Episode 350: Total Reward = -2956.2877402256436, Epsilon = 0.1730128104744653\n",
      "Episode 360: Total Reward = -2939.4643948312123, Epsilon = 0.16455423674261854\n",
      "Episode 370: Total Reward = -2953.2903561160147, Epsilon = 0.15650920157696743\n",
      "Episode 380: Total Reward = -2938.526340747975, Epsilon = 0.14885748713096328\n",
      "Episode 390: Total Reward = -2958.406317011897, Epsilon = 0.14157986400593744\n",
      "Episode 400: Total Reward = -2947.5233540568493, Epsilon = 0.1346580429260134\n",
      "Episode 410: Total Reward = -2953.416242096002, Epsilon = 0.12807462877562611\n",
      "Episode 420: Total Reward = -2944.5706198071402, Epsilon = 0.12181307688414106\n",
      "Episode 430: Total Reward = -2963.2258272879985, Epsilon = 0.11585765144771248\n",
      "Episode 440: Total Reward = -2947.6184840168266, Epsilon = 0.11019338598389174\n",
      "Episode 450: Total Reward = -2963.387413207952, Epsilon = 0.10480604571960442\n",
      "Episode 460: Total Reward = -2931.5020696656097, Epsilon = 0.0996820918179746\n",
      "Episode 470: Total Reward = -2928.544956596004, Epsilon = 0.09480864735409487\n",
      "Episode 480: Total Reward = -2968.6201512639254, Epsilon = 0.09017346495423652\n",
      "Episode 490: Total Reward = -2945.512723010446, Epsilon = 0.08576489601717459\n",
      "Episode 500: Total Reward = -2948.4110610400285, Epsilon = 0.08157186144027828\n"
     ]
    }
   ],
   "source": [
    "env = FormulaOneRacingEnv(dataFrame)\n",
    "\n",
    "# Initialize the agent\n",
    "state_dim = len(env.reset())  # Make sure the reset method returns the initial state\n",
    "action_dim = (env.lap_count, env.num_tire_types)  # Number of laps x Number of tire choices (1 for no action, 5 for tires)\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "# Training hyperparameters\n",
    "episodes = 500\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Agent selects an action\n",
    "        lap_decision, tire_choice = agent.act(state, epsilon)\n",
    "        \n",
    "        # Environment executes the action\n",
    "        next_state, reward, done, _ = env.step((lap_decision, tire_choice))\n",
    "        \n",
    "        # Agent remembers the experience\n",
    "        agent.remember(state, (lap_decision, tire_choice), reward, next_state, done)\n",
    "        \n",
    "        # Agent learns from experiences\n",
    "        agent.replay()\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "    \n",
    "    if (episode+1) % 10 == 0:\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, episodes=100):\n",
    "    total_rewards = []\n",
    "    decisions = []  # List to store decisions of all episodes\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        episode_decisions = []  # List to store decisions for this episode\n",
    "\n",
    "        while not done:\n",
    "            # Always use the best action determined by the policy\n",
    "            lap_decision, tire_choice = agent.act(state, epsilon=0)\n",
    "            next_state, reward, done, _ = env.step((lap_decision, tire_choice))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Record the decision if a pit stop is made\n",
    "            if tire_choice != 0:  # Assuming 0 means 'no action' for tire choicesqq\n",
    "                episode_decisions.append((lap_decision, tire_choice))\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        decisions.append(episode_decisions)  # Store decisions of this episode\n",
    "        print(f\"Test Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f\"Average Reward over {episodes} episodes: {average_reward}\")\n",
    "    return average_reward, decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: Total Reward = -2962.2867141047045\n",
      "Test Episode 2: Total Reward = -2962.2867141047045\n",
      "Test Episode 3: Total Reward = -2962.2867141047045\n",
      "Test Episode 4: Total Reward = -2962.2867141047045\n",
      "Test Episode 5: Total Reward = -2962.2867141047045\n",
      "Test Episode 6: Total Reward = -2962.2867141047045\n",
      "Test Episode 7: Total Reward = -2962.2867141047045\n",
      "Test Episode 8: Total Reward = -2962.2867141047045\n",
      "Test Episode 9: Total Reward = -2962.2867141047045\n",
      "Test Episode 10: Total Reward = -2962.2867141047045\n",
      "Test Episode 11: Total Reward = -2962.2867141047045\n",
      "Test Episode 12: Total Reward = -2962.2867141047045\n",
      "Test Episode 13: Total Reward = -2962.2867141047045\n",
      "Test Episode 14: Total Reward = -2962.2867141047045\n",
      "Test Episode 15: Total Reward = -2962.2867141047045\n",
      "Test Episode 16: Total Reward = -2962.2867141047045\n",
      "Test Episode 17: Total Reward = -2962.2867141047045\n",
      "Test Episode 18: Total Reward = -2962.2867141047045\n",
      "Test Episode 19: Total Reward = -2962.2867141047045\n",
      "Test Episode 20: Total Reward = -2962.2867141047045\n",
      "Test Episode 21: Total Reward = -2962.2867141047045\n",
      "Test Episode 22: Total Reward = -2962.2867141047045\n",
      "Test Episode 23: Total Reward = -2962.2867141047045\n",
      "Test Episode 24: Total Reward = -2962.2867141047045\n",
      "Test Episode 25: Total Reward = -2962.2867141047045\n",
      "Test Episode 26: Total Reward = -2962.2867141047045\n",
      "Test Episode 27: Total Reward = -2962.2867141047045\n",
      "Test Episode 28: Total Reward = -2962.2867141047045\n",
      "Test Episode 29: Total Reward = -2962.2867141047045\n",
      "Test Episode 30: Total Reward = -2962.2867141047045\n",
      "Test Episode 31: Total Reward = -2962.2867141047045\n",
      "Test Episode 32: Total Reward = -2962.2867141047045\n",
      "Test Episode 33: Total Reward = -2962.2867141047045\n",
      "Test Episode 34: Total Reward = -2962.2867141047045\n",
      "Test Episode 35: Total Reward = -2962.2867141047045\n",
      "Test Episode 36: Total Reward = -2962.2867141047045\n",
      "Test Episode 37: Total Reward = -2962.2867141047045\n",
      "Test Episode 38: Total Reward = -2962.2867141047045\n",
      "Test Episode 39: Total Reward = -2962.2867141047045\n",
      "Test Episode 40: Total Reward = -2962.2867141047045\n",
      "Test Episode 41: Total Reward = -2962.2867141047045\n",
      "Test Episode 42: Total Reward = -2962.2867141047045\n",
      "Test Episode 43: Total Reward = -2962.2867141047045\n",
      "Test Episode 44: Total Reward = -2962.2867141047045\n",
      "Test Episode 45: Total Reward = -2962.2867141047045\n",
      "Test Episode 46: Total Reward = -2962.2867141047045\n",
      "Test Episode 47: Total Reward = -2962.2867141047045\n",
      "Test Episode 48: Total Reward = -2962.2867141047045\n",
      "Test Episode 49: Total Reward = -2962.2867141047045\n",
      "Test Episode 50: Total Reward = -2962.2867141047045\n",
      "Test Episode 51: Total Reward = -2962.2867141047045\n",
      "Test Episode 52: Total Reward = -2962.2867141047045\n",
      "Test Episode 53: Total Reward = -2962.2867141047045\n",
      "Test Episode 54: Total Reward = -2962.2867141047045\n",
      "Test Episode 55: Total Reward = -2962.2867141047045\n",
      "Test Episode 56: Total Reward = -2962.2867141047045\n",
      "Test Episode 57: Total Reward = -2962.2867141047045\n",
      "Test Episode 58: Total Reward = -2962.2867141047045\n",
      "Test Episode 59: Total Reward = -2962.2867141047045\n",
      "Test Episode 60: Total Reward = -2962.2867141047045\n",
      "Test Episode 61: Total Reward = -2962.2867141047045\n",
      "Test Episode 62: Total Reward = -2962.2867141047045\n",
      "Test Episode 63: Total Reward = -2962.2867141047045\n",
      "Test Episode 64: Total Reward = -2962.2867141047045\n",
      "Test Episode 65: Total Reward = -2962.2867141047045\n",
      "Test Episode 66: Total Reward = -2962.2867141047045\n",
      "Test Episode 67: Total Reward = -2962.2867141047045\n",
      "Test Episode 68: Total Reward = -2962.2867141047045\n",
      "Test Episode 69: Total Reward = -2962.2867141047045\n",
      "Test Episode 70: Total Reward = -2962.2867141047045\n",
      "Test Episode 71: Total Reward = -2962.2867141047045\n",
      "Test Episode 72: Total Reward = -2962.2867141047045\n",
      "Test Episode 73: Total Reward = -2962.2867141047045\n",
      "Test Episode 74: Total Reward = -2962.2867141047045\n",
      "Test Episode 75: Total Reward = -2962.2867141047045\n",
      "Test Episode 76: Total Reward = -2962.2867141047045\n",
      "Test Episode 77: Total Reward = -2962.2867141047045\n",
      "Test Episode 78: Total Reward = -2962.2867141047045\n",
      "Test Episode 79: Total Reward = -2962.2867141047045\n",
      "Test Episode 80: Total Reward = -2962.2867141047045\n",
      "Test Episode 81: Total Reward = -2962.2867141047045\n",
      "Test Episode 82: Total Reward = -2962.2867141047045\n",
      "Test Episode 83: Total Reward = -2962.2867141047045\n",
      "Test Episode 84: Total Reward = -2962.2867141047045\n",
      "Test Episode 85: Total Reward = -2962.2867141047045\n",
      "Test Episode 86: Total Reward = -2962.2867141047045\n",
      "Test Episode 87: Total Reward = -2962.2867141047045\n",
      "Test Episode 88: Total Reward = -2962.2867141047045\n",
      "Test Episode 89: Total Reward = -2962.2867141047045\n",
      "Test Episode 90: Total Reward = -2962.2867141047045\n",
      "Test Episode 91: Total Reward = -2962.2867141047045\n",
      "Test Episode 92: Total Reward = -2962.2867141047045\n",
      "Test Episode 93: Total Reward = -2962.2867141047045\n",
      "Test Episode 94: Total Reward = -2962.2867141047045\n",
      "Test Episode 95: Total Reward = -2962.2867141047045\n",
      "Test Episode 96: Total Reward = -2962.2867141047045\n",
      "Test Episode 97: Total Reward = -2962.2867141047045\n",
      "Test Episode 98: Total Reward = -2962.2867141047045\n",
      "Test Episode 99: Total Reward = -2962.2867141047045\n",
      "Test Episode 100: Total Reward = -2962.2867141047045\n",
      "Average Reward over 100 episodes: -2962.286714104703\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the testing\n",
    "_, decisions = test_agent(env, agent, episodes=100)\n",
    "\n",
    "# Flatten decisions to plot across all episodes\n",
    "all_decisions = [decision for episode in decisions for decision in episode]\n",
    "laps, tire_choices = zip(*all_decisions)\n",
    "\n",
    "# Plotting the decisions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(laps, tire_choices, alpha=0.6)\n",
    "plt.title('Agent Pit Stop Decisions')\n",
    "plt.xlabel('Lap Number')\n",
    "plt.ylabel('Tire Choice')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
