{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYRE_MAP = {'HYPERSOFT': 1, 'ULTRASOFT': 1, 'SUPERSOFT': 1, 'SOFT': 2, 'MEDIUM': 3, 'HARD': 4, 'INTERMEDIATE': 5, 'WET': 6, 'nan': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaOneRacingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Markov Decision Process model for Formula 1 race strategy, incorporating detailed race data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FormulaOneRacingEnv, self).__init__()\n",
    "        # Define the state space\n",
    "        self.lap_count = 70  # Total laps\n",
    "        self.current_lap = 0\n",
    "        self.tire_wear = 0  # Tire wear percentage\n",
    "        self.fuel_level = 100  # Fuel level percentage\n",
    "        self.tire_age = 0  # Age of the tire in laps\n",
    "        self.gap_to_front = 0  # Gap in seconds to the car in front\n",
    "        self.gap_to_behind = 0  # Gap in seconds to the car behind\n",
    "        self.position = 1  # Current position in the race\n",
    "        self.fcy = 0  # Full course yellow flag (0: no, 1: yes)\n",
    "        self.drs_available = 0  # DRS availability (0: no, 1: yes)\n",
    "\n",
    "        # Define the action space (0: no pit, 1: pit for soft, 2: pit for medium, 3: pit for hard)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Define the state space\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'current_lap': gym.spaces.Discrete(self.lap_count),\n",
    "            'tire_wear': gym.spaces.Box(low=0, high=100, shape=(1,), dtype=np.float32),\n",
    "            'fuel_level': gym.spaces.Box(low=0, high=100, shape=(1,), dtype=np.float32),\n",
    "            'tire_age': gym.spaces.Discrete(100),\n",
    "            'gap_to_front': gym.spaces.Box(low=0, high=120, shape=(1,), dtype=np.float32),\n",
    "            'gap_to_behind': gym.spaces.Box(low=0, high=120, shape=(1,), dtype=np.float32),\n",
    "            'position': gym.spaces.Discrete(20),\n",
    "            'fcy': gym.spaces.Discrete(2),\n",
    "            'drs_available': gym.spaces.Discrete(2)\n",
    "        })\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        # Simulate tire wear and fuel consumption\n",
    "        tire_degradation = np.random.normal(2, 0.5)\n",
    "        fuel_consumption = np.random.normal(1.5, 0.2)\n",
    "\n",
    "        # Handle action effects\n",
    "        if action == 0:\n",
    "            # No pit stop\n",
    "            self.tire_age += 1\n",
    "            self.tire_wear += tire_degradation\n",
    "            self.fuel_level -= fuel_consumption\n",
    "        else:\n",
    "            # Pit stop for new tires\n",
    "            self.tire_wear = 0\n",
    "            self.tire_age = 0\n",
    "            self.fuel_level -= 5  # Pit stop fuel usage\n",
    "            self.fuel_level = min(self.fuel_level + 30, 100)  # Refuel\n",
    "            reward += -30  # Time penalty for pitting\n",
    "            if action in [4, 5]:  # Change tire type based on action\n",
    "                reward += -20  # Additional time penalty for changing to specialty tires\n",
    "\n",
    "        # Update lap and check for race end\n",
    "        self.current_lap += 1\n",
    "        if self.current_lap >= self.lap_count:\n",
    "            done = True\n",
    "        \n",
    "        # Reward calculation\n",
    "        reward += max(0, 100 - self.tire_wear - (100 - self.fuel_level))\n",
    "        reward += -abs(self.gap_to_front)  # Minimize the gap to the front car\n",
    "\n",
    "        # Define next state\n",
    "        next_state = {\n",
    "            'current_lap': self.current_lap,\n",
    "            'tire_wear': self.tire_wear,\n",
    "            'fuel_level': self.fuel_level,\n",
    "            'tire_age': self.tire_age,\n",
    "            'gap_to_front': self.gap_to_front,\n",
    "            'gap_to_behind': self.gap_to_behind,\n",
    "            'position': self.position,\n",
    "            'fcy': self.fcy,\n",
    "            'drs_available': self.drs_available\n",
    "        }\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def reset(self, lap_count):\n",
    "        self.lap_count = lap_count\n",
    "        self.current_lap = 0\n",
    "        self.tire_wear = 0\n",
    "        self.fuel_level = 100\n",
    "        self.tire_age = 0\n",
    "        self.gap_to_front = np.random.uniform(0, 10)\n",
    "        self.gap_to_behind = np.random.uniform(0, 10)\n",
    "        self.position = np.random.randint(1, 21)\n",
    "        self.fcy = np.random.choice([0, 1])\n",
    "        self.drs_available = np.random.choice([0, 1])\n",
    "        return {\n",
    "            'current_lap': self.current_lap,\n",
    "            'tire_wear': self.tire_wear,\n",
    "            'fuel_level': self.fuel_level,\n",
    "            'tire_age': self.tire_age,\n",
    "            'gap_to_front': self.gap_to_front,\n",
    "            'gap_to_behind': self.gap_to_behind,\n",
    "            'position': self.position,\n",
    "            'fcy': self.fcy,\n",
    "            'drs_available': self.drs_available\n",
    "        }\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, num_tire_types, max_laps, hidden_dim=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer for tire type\n",
    "        self.tire_out = nn.Linear(hidden_dim, num_tire_types)\n",
    "        \n",
    "        # Output layer for lap number\n",
    "        self.lap_out = nn.Linear(hidden_dim, max_laps)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        tire_type = self.tire_out(x)\n",
    "        lap_number = self.lap_out(x)\n",
    "        \n",
    "        return tire_type, lap_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, buffer_size=10000, batch_size=64):\n",
    "        self.action_dim = action_dim\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon=0.1):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_network(state)\n",
    "            action = q_values.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randint(0, self.action_dim - 1)\n",
    "        return action\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q = self.q_network(next_states).max(1)[0]\n",
    "        expected_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "\n",
    "        loss = self.criterion(current_q, expected_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FormulaOneRacingEnv()\n",
    "agent = DQNAgent(state_dim=9, action_dim=4)  # Update dimensions appropriately\n",
    "\n",
    "episodes = 500\n",
    "for episode in range(episodes):\n",
    "    state = env.reset(70)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(np.array(list(state.values())))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.add_to_memory(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    print(f\"Episode: {episode + 1}, Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
